{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.transforms import ScaledTranslation, IdentityTransform\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Data augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataloaders\n",
    "train_set = datasets.ImageFolder(\".../xray_data_ternary/train\", train_transforms)\n",
    "test_set = datasets.ImageFolder(\".../xray_data_ternary/test\", test_transforms)\n",
    "\n",
    "# Count number of instances per class\n",
    "samples_count = torch.unique(torch.tensor(train_set.targets), return_counts=True)\n",
    "\n",
    "for i, sample in enumerate(samples_count[0]):\n",
    "    print(f'{train_set.classes[i]} ({samples_count[0][i]}) - {samples_count[1][i]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    train_losses = np.empty(0)\n",
    "    train_accuracies = np.empty(0)\n",
    "    val_losses = np.empty(0)\n",
    "    val_accuracies = np.empty(0)\n",
    "\n",
    "    # Initialize best model weights and best validation accuracy.\n",
    "    best_model_wts = model.state_dict()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_nr, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            train_loss_sum += loss.item() * inputs.size(0)\n",
    "            train_correct += torch.sum(preds == labels)\n",
    "            train_total += labels.size(0)\n",
    "            train_loss = train_loss_sum / train_total\n",
    "            train_acc = train_correct / train_total\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clear\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Print the epoch and loss\n",
    "            print('\\r', f'Epoch {epoch + 1} Batch {batch_nr + 1}/{len(train_loader)} - Train loss: {train_loss} - Accuracy: {train_acc:.2f}', end='')\n",
    "        print('')\n",
    "\n",
    "        # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "        train_losses = np.append(train_losses, train_loss)\n",
    "        train_accuracies = np.append(train_accuracies, train_acc.detach().cpu())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_loss_sum = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        y_pred = torch.empty((0,), dtype=torch.int64).to(device)\n",
    "        y_true = torch.empty((0,), dtype=torch.int64).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_nr, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "                if torch.sum(preds) == 0:\n",
    "                    print(\"\\nAll 0 again\")\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss_sum += loss.item() * inputs.size(0)\n",
    "                val_correct += torch.sum(preds == labels)\n",
    "                val_total += labels.size(0)\n",
    "                val_loss = val_loss_sum / val_total\n",
    "                val_acc = val_correct / val_total\n",
    "\n",
    "                y_pred = torch.cat((y_pred, preds), 0)\n",
    "                y_true = torch.cat((y_true, labels), 0)\n",
    "\n",
    "                # Print the epoch and loss\n",
    "                print('\\r', f'Epoch {epoch + 1} Batch {batch_nr + 1}/{len(val_loader)} - Validation loss: {val_loss} - Accuracy: {val_acc:2.2f}', end='')\n",
    "            print('')\n",
    "\n",
    "            # Add the loss to the total epoch loss\n",
    "            val_losses = np.append(val_losses, val_loss)\n",
    "            val_accuracies = np.append(val_accuracies, val_acc.detach().cpu())\n",
    "\n",
    "            # Update best model\n",
    "            if val_acc < min(val_accuracies):\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, (train_losses, train_accuracies), (val_losses, val_accuracies, y_pred.detach().cpu().numpy(), y_true.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "def test_model(model, criterion, test_loader):\n",
    "    test_losses = np.empty(0)\n",
    "    y_pred = torch.empty((0,), dtype=torch.int64).to(device)\n",
    "    y_true = torch.empty((0,), dtype=torch.int64).to(device)\n",
    "\n",
    "    test_loss_sum = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_nr, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss_sum += loss.item() * inputs.size(0)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += torch.sum(preds == labels)\n",
    "            test_loss = test_loss_sum / test_total\n",
    "            test_acc = test_correct / test_total\n",
    "\n",
    "            y_pred = torch.cat((y_pred, preds), 0)\n",
    "            y_true = torch.cat((y_true, labels), 0)\n",
    "\n",
    "            # Print the epoch and loss\n",
    "            print('\\r', f'Batch {batch_nr + 1}/{len(test_loader)} - Test loss: {test_loss} - Accuracy: {test_acc:2.2f}', end='')\n",
    "        print('')\n",
    "\n",
    "    # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "    test_losses = np.append(test_losses, test_loss)\n",
    "\n",
    "    # Print the accuracy and loss\n",
    "    print(f'Test loss: {test_loss} - Accuracy: {test_acc:.2f}')\n",
    "\n",
    "    return y_pred.detach().cpu().numpy(), y_true.detach().cpu().numpy(), test_losses\n",
    "\n",
    "\n",
    "def plot_results(losses, accuracies, y_pred, y_true):\n",
    "    \"\"\"\n",
    "    :param losses: List of tuples of [0] loss array-like and [1] a label\n",
    "    :param accuracies: List of tuples of [0] accuracy array-like and [1] a label\n",
    "    \"\"\"\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    # Losses\n",
    "    plt.figure(figsize=(10, 5), dpi=150)\n",
    "    for l in losses:\n",
    "        plt.plot(l[0], \"o:\", label=l[1])\n",
    "    plt.xlabel(\"No. of Epochs\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Loss\", fontdict={'size': 16})\n",
    "    plt.xticks(range(0, len(losses[0][0]) + 1, 5), range(0, len(losses[0][0]) + 1, 5), fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracies\n",
    "    plt.figure(figsize=(10, 5), dpi=150)\n",
    "    for a in accuracies:\n",
    "        plt.plot(a[0], \"o:\", label=a[1])\n",
    "    plt.xlabel(\"No. of Epochs\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Accuracy\", fontdict={'size': 16})\n",
    "    plt.xticks(range(0, len(accuracies[0][0]) + 1, 5), range(0, len(accuracies[0][0]) + 1, 5), fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # Define the label names for the plot\n",
    "    categories = ['Normal', 'Bacterial\\nPneumonia', 'Viral\\nPneumonia']\n",
    "    # Compute precision, recall, and F1 scores\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(cf_matrix, annot=True, fmt='g', linewidth=.5, cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "    #Fix annotation positions\n",
    "    for t in ax.texts:\n",
    "        trans = t.get_transform()\n",
    "        offs = ScaledTranslation(0, -0.3, IdentityTransform())\n",
    "        t.set_transform(offs + trans)\n",
    "    # Add the x and y axis labels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=16)\n",
    "    ax.set_ylabel('True Label', fontsize=16)\n",
    "    # Add precision, recall, and F1 scores as annotations to the cells\n",
    "    for i in range(len(categories)):\n",
    "        for j in range(len(categories)):\n",
    "            text = f'P={precision[j]:2.2f}\\nR={recall[i]:2.2f}\\nF1={f1[j]:2.2f}'\n",
    "            ax.text(j + 0.5, i + 0.5, text, ha='center', va='top', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 15\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_predictions = np.empty(0)\n",
    "val_labels = np.empty(0)\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(train_set)):\n",
    "    print(f'Fold no: {fold_idx + 1} -----------------------------------------------------------------')\n",
    "    # Shuffle indices to mix up class order\n",
    "    random.shuffle(train_idx)\n",
    "    random.shuffle(val_idx)\n",
    "\n",
    "    # Create data loaders for the training and validation sets using the samplers\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=train_idx)\n",
    "    val_loader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=val_idx)\n",
    "\n",
    "    fold_train_targets = [train_set.targets[i] for i in train_idx]\n",
    "    class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=[0, 1, 2], y=fold_train_targets), dtype=torch.float).to(device)\n",
    "\n",
    "    # Model setup\n",
    "    model = models.resnet18(weights=\"DEFAULT\")\n",
    "    model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 3))\n",
    "    model = model.to(device)\n",
    "    # Define our loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Define our optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model, train_stats, val_stats = train_model(model, criterion, optimizer, train_loader, val_loader, EPOCHS)\n",
    "\n",
    "    # Visualisation\n",
    "    val_loss, val_acc, val_pred, val_true = val_stats\n",
    "    train_loss, train_acc = train_stats\n",
    "    # Save to list for later averaging\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_predictions = np.append(val_predictions, val_pred)\n",
    "    val_labels = np.append(val_labels, val_true)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    plot_results([(train_loss, \"Training\"), (val_loss, \"Validation\")], [(train_acc, \"Training\"), (val_acc, \"Validation\")], val_pred, val_true)\n",
    "\n",
    "# Aggregated visualisation of average loss & accuracy\n",
    "train_loss_avg = sum(train_losses) / len(train_losses)\n",
    "train_acc_avg = sum(train_accuracies) / len(train_accuracies)\n",
    "val_loss_avg = sum(val_losses) / len(val_losses)\n",
    "val_acc_avg = sum(val_accuracies) / len(val_accuracies)\n",
    "print('Average results across folds: -------------------------------------------------------------------')\n",
    "plot_results([(train_loss_avg, \"Training\"), (val_loss_avg, \"Validation\")], [(train_acc_avg, \"Training\"), (val_acc_avg, \"Validation\")], val_predictions,\n",
    "             val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results([(train_loss_avg, \"Training\"), (val_loss_avg, \"Validation\")], [(train_acc_avg, \"Training\"), (val_acc_avg, \"Validation\")], val_predictions,\n",
    "             val_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train on the full training set\n",
    "def train_model_full(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    train_losses = np.empty(0)\n",
    "    train_accuracies = np.empty(0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_nr, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            train_loss_sum += loss.item() * inputs.size(0)\n",
    "            train_correct += torch.sum(preds == labels)\n",
    "            train_total += labels.size(0)\n",
    "            train_loss = train_loss_sum / train_total\n",
    "            train_acc = train_correct / train_total\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clear\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Print the epoch and loss\n",
    "            print('\\r', f'Epoch {epoch + 1} Batch {batch_nr + 1}/{len(train_loader)} - Train loss: {train_loss} - Accuracy: {train_acc:2.2f}', end='')\n",
    "        print('')\n",
    "\n",
    "        # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "        train_losses = np.append(train_losses, train_loss)\n",
    "        train_accuracies = np.append(train_accuracies, train_acc.detach().cpu())\n",
    "\n",
    "    # Save last model weights\n",
    "    torch.save(model.state_dict(), \"resnet_weights.pt\")\n",
    "\n",
    "    return model, (train_losses, train_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=[0, 1, 2], y=train_set.targets), dtype=torch.float).to(device)\n",
    "# Model setup\n",
    "full_model = models.resnet18(weights=\"DEFAULT\")\n",
    "full_model.fc = nn.Sequential(nn.Linear(full_model.fc.in_features, 3))\n",
    "full_model = full_model.to(device)\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.Adam(full_model.parameters(), lr=0.0001)\n",
    "full_model, train_stats = train_model_full(full_model, criterion, optimizer, train_loader, num_epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "y_pred, y_true, test_loss = test_model(full_model, criterion, test_loader)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# Define the label names for the plot\n",
    "categories = ['Normal', 'Bacterial\\nPneumonia', 'Viral\\nPneumonia']\n",
    "# Compute precision, recall, and F1 scores\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "# Create heatmap\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='g', linewidth=.5, cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "#Fix annotation positions\n",
    "for t in ax.texts:\n",
    "    trans = t.get_transform()\n",
    "    offs = ScaledTranslation(0, -0.3, IdentityTransform())\n",
    "    t.set_transform(offs + trans)\n",
    "# Add the x and y axis labels\n",
    "ax.set_xlabel('Predicted Label', fontsize=16)\n",
    "ax.set_ylabel('True Label', fontsize=16)\n",
    "# Add precision, recall, and F1 scores as annotations to the cells\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(categories)):\n",
    "        text = f'P={precision[j]:.2f}\\nR={recall[i]:.2f}\\nF1={f1[j]:.2f}'\n",
    "        ax.text(j + 0.5, i + 0.5, text, ha='center', va='top', color='black')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Micro average F1-score: {f1_score(y_true, y_pred, average='micro'):.2f}\")\n",
    "print(f\"Macro average F1-score: {f1_score(y_true, y_pred, average='macro'):.2f}\")\n",
    "print(f\"Weighted average F1-score: {f1_score(y_true, y_pred, average='weighted'):.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "batch_tensor = next(iter(train_loader))\n",
    "image_tensor = batch_tensor[0].to(device)\n",
    "label_tensor = batch_tensor[1]\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "image_tensor -= image_tensor.min(3, keepdim=True)[0]\n",
    "image_tensor /= image_tensor.max(3, keepdim=True)[0]\n",
    "\n",
    "model = models.resnet18(weights=\"DEFAULT\")\n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 3))\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('resnet_weights.pt', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "target_layers = [model.layer4[-1]]\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=1)\n",
    "\n",
    "image_tensor = image_tensor.cpu()\n",
    "prediction_tensor = torch.argmax(model(image_tensor.to(device)), 1)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "fig.suptitle(f'ResNet18 GradCAM Visualization')\n",
    "\n",
    "position = 1\n",
    "for i in range(10):\n",
    "    # Define activation heatmap\n",
    "    grayscale_cam = cam(input_tensor=image_tensor[i].unsqueeze(0), targets=None)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    #Select and plot one sample from batch\n",
    "    img = image_tensor[i].numpy().transpose(1, 2, 0)\n",
    "    label = label_tensor[i]\n",
    "    prediction = prediction_tensor[i]\n",
    "\n",
    "    ax = plt.subplot(2, 5, position)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    visualizations = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(visualizations)\n",
    "    plt.title(f'Ground truth: {train_set.classes[label]}\\n Prediction: {train_set.classes[prediction]}')\n",
    "\n",
    "    position += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
